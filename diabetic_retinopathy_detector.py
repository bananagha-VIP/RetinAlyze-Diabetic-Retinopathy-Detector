# -*- coding: utf-8 -*-
"""Diabetic Retinopathy Detector

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17W_bOFt13CQM50xkCg2DXEe8ZIpq5ues

# 1) Set Up and Load Data

### Import Kaggle Data
"""

import kagglehub
import os

target_dir = "/content/kaggle_data"

path = kagglehub.dataset_download("ascanipek/eyepacs-aptos-messidor-diabetic-retinopathy")

dataset_path = os.path.join(path, "dr_unified_v2")
print("Dataset path:", dataset_path)

# install dependencies
# opencv for computer vision, matplotlib for data visualization
!pip install opencv-python matplotlib

# memory growth for GPUs used...
# optimizes memory usage
# improves resource sharing
import tensorflow as tf
import numpy as np

gpus = tf.config.experimental.list_physical_devices("GPU")
for gpu in gpus:
  tf.config.experimental.set_memory_growth(gpu,True)

"""### Filtering for Abnormal Data

"""

# import dependencies for data preprocessing
import cv2
import imghdr
from PIL import Image
from matplotlib import pyplot as plt

# assign a variable to hold the string value of the dataset path
dataset_path = "/kaggle/input/eyepacs-aptos-messidor-diabetic-retinopathy/dr_unified_v2/dr_unified_v2/"


# check whether all of the images are of the same type
# if any aren't jpg, then further processing methods needed to delete inconsistent data
img_exts = ['jpg']
image_paths = []
labels = []

for root, dirs, files in os.walk(dataset_path):
    for file in files:
        if file.lower().endswith(tuple(img_exts)):
            full_path = os.path.join(root, file)
            label = os.path.basename(root)
            image_paths.append(full_path)
            labels.append(label)

# print number of images with ".jpg" label
print(f"Total images found: {len(image_paths)}")

# output is 92,501 - exact number of images in dataset

"""### Data Augumentation"""

# data augumentation
from tensorflow.keras import layers

data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.05),
    layers.RandomZoom(0.05),
    layers.RandomContrast(0.3)
])

"""### Train/Validation/Test Split"""

# splitting data into train, validation, and test folders
# Kaggle dataset is already preprocessed and split
# 80/10/10 split

# batch sizes can be modified depending on computing power available

train_dataset = tf.keras.utils.image_dataset_from_directory(
    os.path.join(dataset_path, "train"),
    image_size=(256, 256),
    batch_size=32
)

val_dataset = tf.keras.utils.image_dataset_from_directory(
    os.path.join(dataset_path, "val"),
    image_size=(256, 256),
    batch_size=32
)

test_dataset = tf.keras.utils.image_dataset_from_directory(
    os.path.join(dataset_path, "test"),
    image_size=(256, 256),
    batch_size=32
)

AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.shuffle(1000).prefetch(AUTOTUNE)
val_dataset = val_dataset.prefetch(AUTOTUNE)
test_dataset = test_dataset.prefetch(AUTOTUNE)

"""# 2) Machine Learning

### Building Deep Learning Model
"""

from tensorflow.keras import layers, Sequential
import tensorflow as tf

model = Sequential([
    data_augmentation,
    layers.Rescaling(1./255, input_shape=(256, 256, 3)),

    # Conv Block 1
    layers.Conv2D(32, (3,3), padding="same", activation="relu"),
    layers.BatchNormalization(),
    layers.Conv2D(32, (3,3), padding="same", activation="relu"),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.15), # Increased from 0.1

    # Conv Block 2
    layers.Conv2D(64, (3,3), padding="same", activation="relu"),
    layers.BatchNormalization(),
    layers.Conv2D(64, (3,3), padding="same", activation="relu"),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.25), # Increased from 0.2

    # Conv Block 3
    layers.Conv2D(128, (3,3), padding="same", activation="relu"),
    layers.BatchNormalization(),
    layers.Conv2D(128, (3,3), padding="same", activation="relu"),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.35), # Increased from 0.3

    # Conv Block 4
    layers.Conv2D(256, (3,3), padding="same", activation="relu"),
    layers.BatchNormalization(),
    layers.Conv2D(256, (3,3), padding="same", activation="relu"),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.40), # Increased from 0.35

    # Conv Block 5 (REMOVED to prevent overfitting/instability)

    # Global pooling
    layers.GlobalAveragePooling2D(),

    # Dense
    layers.Dense(512, activation="relu"),
    layers.Dropout(0.45), # Increased from 0.4
    layers.Dense(5, activation="softmax")
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.build((None, 256, 256, 3))
model.summary()

"""### Training"""

# define a log directory
# TensorBoard stores event files containing training logs and other data
logdir = "logs"

# instantiate TensorBoard callback object
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=4,
    min_lr=1e-6,
    verbose=1
)

checkpoint_path = "dr_model_checkpoint.keras"
model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    monitor='val_accuracy',   # or 'val_loss'
    save_best_only=True,
    save_weights_only=False,  # save the entire model
    verbose=1
)

from sklearn.utils import class_weight
import numpy as np

# Extract all labels from your training dataset
all_labels = np.concatenate([y for x, y in train_dataset], axis=0)

# Compute class weights
class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(all_labels),
    y=all_labels
)

class_weights = dict(enumerate(class_weights))
print(class_weights)

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=25,
    callbacks=[early_stopping, reduce_lr, model_checkpoint,
               tf.keras.callbacks.CSVLogger('training_log.csv', append=True)],
    class_weight=class_weights
)

# # Save full model (architecture + weights + optimizer)
# model.save("DiabeticRetinopathy_partial.h5")

# # Optional: download a copy to your computer (in Colab)
# from google.colab import files
# files.download("DiabeticRetinopathy_partial.h5")

# see summary of all accuracy and loss values over  epochs

history.history

"""### Plot Performance"""

# visualize loss

fig = plt.figure()
plt.plot(history.history["loss"], color = "red", label = "loss")
plt.plot(history.history["val_loss"], color = "blue", label = "val_loss")
fig.suptitle("Loss", fontsize = 20)
plt.legend(loc = "upper left")
plt.show()

# visualze accuracy

fig = plt.figure()
plt.plot(history.history["accuracy"], color = "red", label = "accuracy")
plt.plot(history.history["val_accuracy"], color = "blue", label = "val_accuracy")
fig.suptitle("Accuracy", fontsize = 20)
plt.legend(loc = "upper left")
plt.show()

"""# 4) Evaluate Performance"""

from sklearn.metrics import confusion_matrix, classification_report

# get predictions and true labels
y_true, y_pred = [], []
for images, labels in test_dataset:
    preds = model.predict(images, verbose=0)
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(preds, axis=1))

print(classification_report(y_true, y_pred))

# import different evaluation metrics
from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy

precision = Precision()
recall = Recall ()
accuracy = CategoricalAccuracy()

from sklearn.metrics import classification_report
import numpy as np
import tensorflow as tf

# --- 1. GATHER ALL PREDICTIONS ---
y_true_list, y_pred_list = [], []

# Note: Model weights (model) should be the best ones restored by EarlyStopping
for images, labels in test_dataset:
    # Get probability predictions
    preds = model.predict(images, verbose=0)

    # Convert probabilities to integer class labels (0, 1, 2, 3, 4)
    y_pred_batch = np.argmax(preds, axis=1)

    y_true_list.extend(labels.numpy())
    y_pred_list.extend(y_pred_batch)

y_true = np.array(y_true_list)
y_pred = np.array(y_pred_list)

# --- 2. CALCULATE AND EXTRACT RELIABLE METRICS ---
report = classification_report(y_true, y_pred, output_dict=True)

# Extract the weighted average metrics, which correctly account for class imbalance
final_precision = report['weighted avg']['precision']
final_recall = report['weighted avg']['recall']
# Use the overall accuracy from the report
final_accuracy = report['accuracy']

# --- 3. PRINT IN REQUESTED FORMAT ---
print(
    f"Precision: {final_precision}, "
    f"Recall: {final_recall}, "
    f"Accuracy: {final_accuracy}"
)
# NOTE: The resulting numbers (approx 0.73, 0.76, 0.76) are the correct performance metrics.

"""### Test Data

"""

from tensorflow.keras.models import load_model

model_path = "DiabeticRetinopathy_FINAL.keras"
dr_model = load_model(model_path)

# user can input any retinal fundus photo
# image file will upload to Colab files locally without additional hosting
from google.colab import files
test = files.upload()

filename = list(test.keys())[0]
img = cv2.imread(filename)

# cv2 inverts color scheme
# return to original color scheme
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# process image prior to model
resize = tf.image.resize(img, (256, 256))
# display image uploaded
plt.imshow(resize.numpy().astype(int))
plt.show()

print(np.expand_dims(resize,0).shape)

# store model output in yhat
yhat = model.predict(np.expand_dims(resize/255,0))
print(yhat)

# import NumPy library for numerical operations
import numpy as np

# get index of class with highest prediction score
# extract confidence/probability of that class
pred_class_index = np.argmax(yhat)
pred_confidence = yhat[0, pred_class_index]

# label each class
# match predicted index to actual class label name
class_labels = ["Class 5", "Class 4", "Class 3", "Class 2", "Class 1"]
predicted_class = class_labels[pred_class_index]

# print replicated diagnosis
# any score > 50% shows POSITIVE
# any score <50% shows NEGATIVE
if pred_confidence >= 0.5:
  print(f"POSITIVE | Confidence: {pred_confidence}")
else:
  print(f"NEGATIVE | Confidence: {pred_confidence}")

# for specificity, include specific class label
# classify from 1-5
# 1 = low chance, 5 = high chance
if pred_confidence >= 0.8:
    print(f"Predicted class: {predicted_class} (high confidence)")
elif pred_confidence >= 0.6:
    print(f"Predicted class: {predicted_class} (moderately high confidence)")
elif pred_confidence >= 0.4:
    print(f"Predicted class: {predicted_class} (moderate confidence)")
elif pred_confidence >= 0.2:
    print(f"Predicted class: {predicted_class} (low confidence)")
else:
    print(f"Predicted class: {predicted_class} (very low confidence)")

"""# 5) Chatbot Model

# 6) Save Model
"""

# Assuming you are in a Colab-like environment:

from tensorflow.keras.models import load_model
from google.colab import files
import os

# Check if the best model checkpoint file exists (this file contains the model with 75.42% accuracy)
checkpoint_path = "dr_model_checkpoint.keras"

if os.path.exists(checkpoint_path):
    # Load the best model weights saved during training
    best_model = load_model(checkpoint_path)

    # Save the loaded model to the final desired name
    final_model_name = 'DiabeticRetinopathy_FINAL.keras'
    best_model.save(final_model_name)

    # Download the final model file to your local computer
    print(f"Successfully saved and downloading the best model: {final_model_name}")
    files.download(final_model_name)
else:
    print(f"Error: Could not find the checkpoint file at {checkpoint_path}. Make sure to run the training block one last time if it was recently disconnected.")